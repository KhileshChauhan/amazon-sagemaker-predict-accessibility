{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict park accessibility in NYC using Machine Learning\n",
    "\n",
    "In this notebook, we work through how we can ingest the parquet file produced by our ETL pipeline into an ML algorithm for training a model to predict accessibility. We will go through the entire data science lifecyle -- \n",
    "\n",
    "data ingest --> data exploration --> data cleaning --> staging the training data --> machine learning.\n",
    "\n",
    "Accessiblity here is defined as whether parks are categorized as Level 4 or not.  \n",
    "\n",
    "For Machine learning, we will use two approaches -- \n",
    "\n",
    "1/ Train an XGBoost model to predict whether accessible parks are available based on individual tax returns and park data \n",
    "\n",
    "2/ Train a model using SageMaker Autopilot and compare the results. SageMaker Autopilot is a fully managed ML service that can train various machine learning models with different hyperparameters, and allow you to pick the best one. Here we will use the SageMaker Python SDK to make API calls to SageMaker Autopilot to train and deploy an ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Data\n",
    "\n",
    "Note that to import the data in parquet format into a pandas dataframe, we will need to install pyarrow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, updated boto3\n",
    "!pip install --upgrade pip\n",
    "if boto3.__version__ != '1.10.33':\n",
    "    !pip install boto3=='1.10.33'\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow==0.12.0\n",
    "#import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'part-00000-588c7835-a113-419a-8cea-580b11e397e8-c000.snappy.parquet' # replace this with your parquet file\n",
    "df = pd.read_parquet(FILENAME, engine ='pyarrow')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Playground_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Dataset = {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning prior to Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several data cleaning steps that need to be performed:\n",
    "\n",
    "1) There isn't enough data to identify each playground Id separately but we can extract the Borough information as a categorical variable <br/>\n",
    "2) We need to convert the AGI to ordinal values <br/>\n",
    "3) We need to convert Accessible and Adaptible Swing to numeric values <br/>\n",
    "4) We will drop the name and location columns <br/>\n",
    "5) We will drop any columns on volunteer prepared taxes <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Borough'] = [str(x)[0] for x in df.Playground_ID.values]\n",
    "convert_cols = {'Accessible': {'Y': 1, 'N': 0},\n",
    "                'Adaptive_Swing': {'Y': 1, 'N': 0}}#,\n",
    "              # 'adjusted_gross_income': {'':0, '$1 under $25,000':1, '$25,000 under $50,000': 2, '$50,000 under $75,000':3, \n",
    "              #                          '$75,000 under $100,000':4, '$100,000 under $200,000':5, '$200,000 or more':6}}\n",
    "df.replace(convert_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLSCONVERT = ['Adaptive_Swing', 'num_of_exemptions',\n",
    "       'Accessible', 'num_of_dependents', 'num_of_returns',\n",
    "       'num_of_joint_returns', 'num_of_head_of_household_returns',\n",
    "       'num_of_single_returns']\n",
    "indexlist = []\n",
    "for col in COLSCONVERT:\n",
    "    indexlist = indexlist + list(df.loc[df[col]=='**'].index)\n",
    "df = df.drop(index= indexlist)\n",
    "df[COLSCONVERT] = df[COLSCONVERT].apply(pd.to_numeric)\n",
    "df.replace({'Level': {None:0, '1':0, '2':0, '3':0, '4':1}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_DROP = ['lat', 'lon', 'Playground_ID', 'Name', 'Location', 'Prop_ID','num_of_volunteer_prepared_returns_Total',\n",
    "                  'num_of_volunteer_prepared_returns_Num_of_volunteer_income_tax_assistance_prepared_returns',\n",
    "                 'num_with_paid_preparers_signature','num_of_volunteer_prepared_returns_Num_of_tax_counseling_for_the_elderly_prepared_returns', \n",
    "                'zipcode','School_ID','Status']\n",
    "df = df.drop(columns = COLS_TO_DROP)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this dataframe at this stage as a separate dataframe for AutoPilot\n",
    "autodf = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark everything with Level 4 as 1, and else 0. Why have we done this?\n",
    "\n",
    "Look at the graph below. It shows that most playgrounds in NYC are classified as Level 4 playgrounds.  Level 4 means Accessible Playgrounds with Transfer Platforms and Ground Level Play Features; and thus represents the most sophisticated playgrounds.\n",
    "\n",
    "We want to predict whether a given Playground ID in a given school district, given demographic information about the income levels of households in that zipcode is a Level 4 playground or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(df.Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Model accuracy = {}\".format(len(df[df['Level']==1])/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Having converted the raw data into numerical format, we can explore the dataset and extract some meaningful information and look for any correlations in the data, which may be indicative of whether income is correlated to playground type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total returns versus AGI shows an overall trend that we need to normalize out. \n",
    "sns.barplot(x='size_of_adjusted_gross_income', y = 'num_of_returns', data = df)\n",
    "plt.title(\"Total Returns versus AGI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of dependents versus number of exceptions\n",
    "sns.regplot(x = 'num_of_dependents', y = 'num_of_exemptions', data = df)\n",
    "plt.title(\"Scatter plot of dependents versus exceptions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce new columns which are the normalized number of joint, head of household and single returns\n",
    "COLS_TO_NORMALIZE = ['num_of_joint_returns', 'num_of_head_of_household_returns',\n",
    "       'num_of_single_returns']\n",
    "for col in COLS_TO_NORMALIZE:\n",
    "    df['normalized_'+ col] = df[col]/df['num_of_returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='size_of_adjusted_gross_income', y = 'normalized_num_of_head_of_household_returns', data = df)\n",
    "plt.title(\"AGI versus normalized HOH returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Accessible', y = 'num_of_dependents', hue = 'Borough', data = df)\n",
    "plt.title(\"Playground Accessibility by number of dependents and Borough\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'Accessible', y = 'normalized_num_of_joint_returns', hue = 'Borough', data = df)\n",
    "plt.title(\"Playground accessibility by normalized number of joint returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not much correlation between adjusted gross income level and number of available playgrounds. Certain zip codes simply\n",
    "# dont have any filers in the high income categories. \n",
    "sns.countplot(x = 'Level' , hue = 'size_of_adjusted_gross_income', data = df)\n",
    "plt.title(\"Distribution of Accessibility Levels by Income Accross all Boroughs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this play out at a Borough specific level? Let's look at Manhattan and Bronx\n",
    "bordf = df[df['Borough']=='X']\n",
    "sns.countplot(x = 'Level', hue = 'size_of_adjusted_gross_income', data = bordf)\n",
    "plt.title(\"Distribution of Accessibility Levels by Income in the Bronx\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'size_of_adjusted_gross_income', hue = 'Level', data = bordf)\n",
    "plt.title(\"Distribution of Accessibility Levels by Income in the Manhattan\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this play out at a Borough specific level? Let's look at Manhattan and Bronx\n",
    "bordf = df[df['Borough']=='M']\n",
    "sns.countplot(x = 'Level', hue = 'size_of_adjusted_gross_income', data = bordf)\n",
    "plt.title(\"Distribution of Accessibility Levels by Income in the Manhattan\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'size_of_adjusted_gross_income', hue = 'Level', data = bordf)\n",
    "plt.title(\"Distribution of Accessibility Levels by Income in the Manhattan\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second plot in each Borough category shows that the availability distribution of playgrounds is almost identical accross all income groups, suggesting that these variables are not strongly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a correlation plot to confirm. Convert the Level column to numeric\n",
    "df['Level'] = df.Level.astype('float32')\n",
    "sns.heatmap(df.corr())\n",
    "plt.title('Correlation Matrix Plot showing different features and their correlation with each other')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "For further preparation of the data for ML, we need to drop some more columns:\n",
    "1) We have subsumed the accessibility column under level, so we can drop that. Remember that anything that was not wheelchair accessible is classifed as Level 0 <br/>\n",
    "\n",
    "2) The adaptive swing column is correlated to Level 0 -- Not accessible, and can help pick that out. <br/> \n",
    "\n",
    "3) We will only keep the normalized columns for joint, HOH and Single returns and drop the unnormalized ones. <br/>\n",
    "\n",
    "4) We will keep the total number of returns column <br/> \n",
    "\n",
    "5) SInce the total returns is a pretty large number, we will standardize the columns as well <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning modeling\n",
    "\n",
    "Here we will try to predict the level of playground based on the data available.\n",
    "For any ML model, we need to provide the label column first. Let's drop some of the fields above and prepare the data for ML training in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.drop(columns =['Accessible', 'num_of_returns', \n",
    "                             'num_of_dependents','num_of_joint_returns', 'num_of_head_of_household_returns',\n",
    "                             'num_of_single_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.get_dummies(final_df)\n",
    "df1 = final_df.Level\n",
    "df2 = final_df.drop(columns = 'Level')\n",
    "final_df = pd.concat([df1, df2], axis = 1)\n",
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(final_df, test_size = 0.2, random_state = 42)\n",
    "sns.countplot(X_test.Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload Training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket() # you can replace with this your processed bucket name <<user-id>>-processed\n",
    "prefix = 'sagemaker/accessibility'\n",
    "\n",
    "train_file = 'train.csv'\n",
    "X_train.to_csv(train_file,index=False,header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "train_data = sagemaker.session.s3_input('s3://{}/{}/train'.format(bucket, prefix), \n",
    "                                        content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': train_data, 'validation': train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SageMaker Autopilot\n",
    "\n",
    "As you can see, training an ML model, feature engineering, preprocessing can take quite some time and be a fairly involved process. Let's see how we can use SageMaker Autopilot, which is SageMaker's built in AutoML feature to take the raw dataset, and train a bunch of ML models, and let AutoPilot take care of the preprocessing, feature engineering and model training all within a few simple API calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by looking at autodf -- we will drop the accessibility column, since we have already incorporated that into Level. Note that to go from autodf to the final training dataset for XGBoost above we had to implement a sequence of feature engineering steps:\n",
    "\n",
    "1) We converted Borough columns to one hot encoding\n",
    "\n",
    "2) We dropped all the non normalized columns which are strongly correlated with the feature engineered normalized ones.\n",
    "\n",
    "3) We had to split the data randomly into training and test datasets which we uploaded separately to S3.\n",
    "\n",
    "4) We had to save the csv files locally without headers and indexes. \n",
    "\n",
    "5) While this is not required for XGBoost, we removed missing values. Linear learner algorithm for example doesn't accept missing values. \n",
    "\n",
    "Autopilot takes care of all the feature engineering steps for us. Simply feed in the initial dataframe as an input, save it as csv, point Autopilot to the dependent variable and it will take care of the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the autodf dataframe again\n",
    "autodf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we have missing values in almost every column. We will drop those as AutoML doesn't support missing values\n",
    "#for now.\n",
    "autodf.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the autodf into s3 directly\n",
    "autodf.dropna(inplace=True)\n",
    "print(autodf.shape)\n",
    "autodf = autodf.sample(frac=1)\n",
    "\n",
    "#Let's take out a random sample for testing and set it aside\n",
    "TRAIN_LENGTH = X_train.shape[0]\n",
    "test_sample = autodf[TRAIN_LENGTH+1:]\n",
    "print(\"Shape of test data = {}\".format(test_sample.shape))\n",
    "test_sample.drop(columns=['Accessible']).to_csv('automl-test.csv', index=False)\n",
    "\n",
    "\n",
    "autodf[:TRAIN_LENGTH].drop(columns=['Accessible']).to_csv('automl-train.csv', index= False)\n",
    "autotrainpath = session.upload_data(path=\"automl-train.csv\", key_prefix=prefix + \"/input\")\n",
    "print(autotrainpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the AutoML job: specify the target attribute name and the location of the input file\n",
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/{}/input'.format(bucket,prefix)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': 'Level'\n",
    "    \n",
    "    }\n",
    "  ]\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': 's3://{}/{}/automloutput'.format(bucket,prefix)\n",
    "  }\n",
    "print(input_data_config)\n",
    "print(output_data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "sm = boto3.Session().client(service_name='sagemaker',region_name=region)\n",
    "auto_ml_job_name = 'automl-dm-' + timestamp_suffix\n",
    "print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(AutoMLJobName=auto_ml_job_name,\n",
    "                      InputDataConfig=input_data_config,\n",
    "                      OutputDataConfig=output_data_config,\n",
    "                      RoleArn=role,\n",
    "                     ProblemType='BinaryClassification',\n",
    "                     AutoMLJobObjective = {'MetricName':'Accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off the Autopilot job\n",
    "import time\n",
    "start = time.time()\n",
    "print ('JobStatus - Secondary Status')\n",
    "print('------------------------------')\n",
    "\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "    print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "    sleep(30)\n",
    "end = time.time()\n",
    "print(\"Time Taken for job = {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "describe_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the best job\n",
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "print(best_candidate)\n",
    "print('\\n')\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now use boto3 to deploy the model\n",
    "model_name = 'automl-' + timestamp_suffix\n",
    "\n",
    "model = sm.create_model(Containers=best_candidate['InferenceContainers'],\n",
    "                            ModelName=model_name,\n",
    "                            ExecutionRoleArn=role)\n",
    "\n",
    "print('Model ARN corresponding to the best candidate is : {}'.format(model['ModelArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = 'automl-endpoint-config' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Endpoint for the AutoML job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'autmlEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model predictions\n",
    "\n",
    "Replace the endpoint name below with your endpoint name from the above cell: it should start with 'autmlEndpoint-yyyy-mm-dd-hh-mm-ss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, csv_deserializer, csv_serializer\n",
    "#endpoint  = 'autmlEndpoint-2019-12-23-17-15-48' #replace with your endpoint name here\n",
    "autopredictor = RealTimePredictor(endpoint = endpoint_name, serializer=csv_serializer, deserializer =csv_deserializer, \n",
    "                                  content_type='text/csv', sagemaker_session = session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_sample.Level\n",
    "X_pred = test_sample.drop(columns = ['Level', 'Accessible']).reset_index().drop(columns = ['index'])\n",
    "y_pred = [int(autopredictor.predict(np.array(X_pred.loc[x]))[0][0]) for x in range(len(X_pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy from AutoML Job = {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "Here We explore two modes of model deployment -- batch and as a live inference endpoint. \n",
    "First we deploy as a live endpoint and obtain model metrics <br/>\n",
    "Next we do a batch transform job. For this we need to load the test data into S3 as well <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "y_test = X_test.Level\n",
    "X_test_drop = X_test.drop(columns = 'Level')\n",
    "for i in range(X_test.shape[0]):\n",
    "    predictions.append(xgb_predictor.predict(np.array(X_test_drop.iloc[i])))\n",
    "predictions = np.array(np.round(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model where we have normalized the features returns a higher accuracy score, but the AutoML does a really good job without virtually any data preparation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Inferences directly in SQL using Amazon Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will ingest the training data csv file into Amazon Athena and perform inferences directly against the XGBoost SageMaker endpoint. Here we will show how the process works for the XGBoost endpoint. \n",
    "\n",
    "To do so, navigate to SageMaker console and go to Endpoints. Identify the endpoint you just created for XGBoost. It should start with 'sagemaker-xgboost-####'. Copy this to Clipboard.\n",
    "\n",
    "Next, navigate to the **Readme** associated with this workshop to the section **Inferences using Amazon Athena** and complete the rest of the workshop. \n",
    "\n",
    "Don't forget to come back to SageMaker and run the 2 cells below, to delete the endpoints you created to avoid paying for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences Using Amazon Athena\n",
    "\n",
    "To be completed from Readme. Since this feature is in **Preview** as of 05-01-2020, it can only be completed if you are in \"us-east-1\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoints (Only do so after completing the Athena portion)\n",
    "\n",
    "See the Readme in the Github for this workshop for next steps on how to complete invoke the model endpoints from Amazon Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autopredictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
